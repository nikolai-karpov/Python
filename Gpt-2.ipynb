{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gpt-2-All-Models",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA3oRLxMRohz"
      },
      "source": [
        "# Use Tensorflow 1\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "# Select GPT-2 model: 117M, 124M, 355M, 774M, 1558M\n",
        "\n",
        "model_name = '117M'     # '117M' - smallest model. Set '1558M' for biggest 1.5B model\n",
        "\n",
        "!git clone https://github.com/openai/gpt-2\n",
        "\n",
        "%cd gpt-2\n",
        "\n",
        "!pip3 install -r requirements.txt\n",
        "\n",
        "# Download GPT-2 (selected model, for example 117M)\n",
        "!python3 download_model.py $model_name\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PB1gYfkeZvU"
      },
      "source": [
        "# Generate samples by user input. Wait for string \"Model prompt >>>\", enter you text (begin phrase for network) and press Enter\n",
        "\n",
        "!python3 src/interactive_conditional_samples.py --model_name=$model_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuKhosKgQHna"
      },
      "source": [
        "# Generate samples to file samples.txt. To stop press STOP button at this cell. How download result samples.txt see last cell\n",
        "\n",
        "!python3 src/generate_unconditional_samples.py --model_name=$model_name | tee samples.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC9_n-4HVmHH"
      },
      "source": [
        "# Generate samples with parameters: top_k and temperature. Result saved to samples.txt\n",
        "\n",
        "!python3 src/generate_unconditional_samples.py --model_name=$model_name --top_k 40 --temperature 0.7 | tee samples.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePBKJi8yZJUO"
      },
      "source": [
        "# Download file samples.txt with generated text (unconditional mode only)\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "files.download('samples.txt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HXYAbJk4rF1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}